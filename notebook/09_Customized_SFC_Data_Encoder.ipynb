{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452ded9c-5dc3-4efb-b8fe-9aff0cb914fc",
   "metadata": {},
   "source": [
    "## Hands on the 3D Convolutional Layers in PyTorch\n",
    "\n",
    "Since our input data is (6,241,121) data array, we may need 3D convolutional layers in the image encoder. Here we use PyTorch [Conv3d](https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d) to build an encoder.vrxiv.org/abs/2205.14100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c181f5f-e775-4e7a-9a16-4e5dcf80a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_PATH = '../../data/ncep_npy/'\n",
    "# Walk through the sub-directories\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "    for name in files:\n",
    "        if name.endswith('.npy'):\n",
    "            date = name.replace('.npy','')\n",
    "            url = os.path.join(root, name)\n",
    "            file_list.append({'date':date, 'furi':url})\n",
    "\n",
    "file_list = pd.DataFrame(file_list)\n",
    "print(file_list.shape)\n",
    "print(file_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3381de-967d-4e8c-b4a2-080699f040de",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/ncep_npy/20110731.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcartopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mccrs\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/ncep_npy/20110731.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_surface_data\u001b[39m(data, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' Visualize the 6x241x121 dataset.  '''\u001b[39;00m\n",
      "File \u001b[1;32mC:\\usr\\anaconda3\\envs\\weathergpt\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/ncep_npy/20110731.npy'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.load('../../data/ncep_npy/20110731.npy')\n",
    "\n",
    "def plot_surface_data(data, title=None):\n",
    "    ''' Visualize the 6x241x121 dataset.  '''\n",
    "    # Retrieve each variable\n",
    "    pwat = data[0,:,:]\n",
    "    slp = data[1,:,:]\n",
    "    rh = data[2,:,:]\n",
    "    t = data[3,:,:]\n",
    "    u = data[4,:,:]\n",
    "    v = data[5,:,:]\n",
    "    # Create coordinate\n",
    "    x = np.linspace(60, 180, 241)\n",
    "    y = np.linspace(0, 60, 121)\n",
    "    x2d, y2d = np.meshgrid(x, y)\n",
    "    # Create map\n",
    "    crs = ccrs.PlateCarree()\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.coastlines(linewidths=0.3, alpha=0.8)\n",
    "    # draw variables\n",
    "    water = ax.contourf(x2d, y2d, rh*100, transform=crs, levels=11, cmap='binary', alpha=0.3, label='RH [%]')\n",
    "    l1 = ax.contour(x2d, y2d, slp*14000+93000, transform=crs, levels=11, colors='black', linestyles='solid', linewidths=0.8, label='P [pa]')\n",
    "    l2 = ax.contour(x2d, y2d, t*100+220, transform=crs, levels=11, colors='black', linestyles='dashed', linewidths=0.5, label='T [K]')\n",
    "    wind = ax.streamplot(x2d, y2d, u*120-60, v*120-60, transform=crs, color='blue')\n",
    "    # draw addons\n",
    "    ax.set_extent([60, 180, 0, 60], crs=crs)\n",
    "    if not title is None:\n",
    "        ax.set_title(title)\n",
    "    plt.show()\n",
    "    #\n",
    "    return(0)\n",
    "\n",
    "plot_surface_data(data, title='20110731-00Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60259f9a-77ba-4939-8fc5-9af58602dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "class MySurfaceDataset(Dataset):\n",
    "    def __init__(self, data_info, npy_dir, transform=None, target_transform=None):\n",
    "        self.data_info = data_info\n",
    "        self.npy_dir = npy_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_info.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.data_info['furi'].iloc[idx]\n",
    "        np_array = np.load(data_path)\n",
    "        data = torch.from_numpy(np_array)\n",
    "        label = self.data_info['date'].iloc[idx]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return data, label\n",
    "\n",
    "ncep_data = MySurfaceDataset(data_info=file_list, npy_dir='../../data/ncep_npy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dbd61-3182-4e1b-901b-2dc28af70f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchinfo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    " \n",
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm2d(6, affine=False),\n",
    "            nn.Conv2d(6, 16, kernel_size=(4, 4), stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16, affine=False),\n",
    "            nn.Conv2d(16, 32, kernel_size=(4, 4), stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 8, kernel_size=(4, 4), stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 32, \n",
    "                               kernel_size=(4, 4), \n",
    "                               stride=3, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, \n",
    "                               kernel_size=(4, 4), \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 6, \n",
    "                               kernel_size=(4, 4), \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    " \n",
    " \n",
    "# Initialize the autoencoder\n",
    "model = Autoencoder()\n",
    " \n",
    "print(model)\n",
    "print(summary(model, input_size=(64, 6, 241, 121)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b8002-3159-401c-80e4-f48c37ef8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the autoencoder\n",
    "model = Autoencoder()\n",
    "\n",
    "# Move the model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    " \n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "# Create data loaders.\n",
    "dataloader = DataLoader(ncep_data, batch_size=batch_size)\n",
    "\n",
    "# Train the autoencoder\n",
    "num_epochs = 10\n",
    "model = model.half()\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 5== 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56af89-15a9-4cba-9dab-e5950543c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), '../../data/conv_autoencoder.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weathergpt",
   "language": "python",
   "name": "weathergpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
